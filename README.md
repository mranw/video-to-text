```markdown
# Video-to-Text Processor with Knowledge Base Generation

Данный проект представляет собой автоматизированное решение для обработки видеофайлов с Яндекс.Диска и подготовки базы знаний из расшифровок курсов Школы Насти Рыбки. На первом этапе проект включает следующие задачи:

- Рекурсивный обход папок на Яндекс.Диске и поиск видеофайлов.
- Скачивание видеофайлов для временной обработки.
- Извлечение аудиодорожки из видео с использованием **ffmpeg** с принудительным преобразованием в **OGG Opus** (моно, 48000 Гц, 64k битрейт).
- Определение длительности аудиофайла с помощью **ffprobe**.
- Загрузка аудиофайла в **Yandex Object Storage**.
- Асинхронное распознавание аудио через [Yandex SpeechKit](https://cloud.yandex.ru/services/speechkit) с использованием API-ключа или IAM-токена.
- Поддержка двух режимов распознавания:
  - **general** – динамический интервал ожидания, вычисляемый по длительности аудио (минимум 10 сек).
  - **deferred-general** – фиксированный интервал опроса (60 сек) с максимальным временем ожидания 24 часа.
- Сохранение распознанного текста в файл с добавлением новой информации (без перезаписи уже существующих данных).
- Отметка обработанных видеофайлов для предотвращения повторного распознавания.
- Демонический режим работы с логированием и обработкой ошибок.
- Интервал сканирования папок на Яндекс.Диске – **12 часов**.

Кроме того, проект включает модуль обработки расшифрованного текста курсов. Текст очищается от лишних символов, разбивается на логические фрагменты (по курсам, разделам, урокам) и структурируется в формате вопрос-ответ с классификацией по уровню значимости (актуальная информация имеет более высокий приоритет). Эта база знаний будет использоваться в дальнейшем для создания AI-ассистента на базе LLaMA.

## Структура проекта

Проект разделён на несколько модулей для повышения удобства поддержки и масштабируемости:

```
project/
├── main.py               # Точка входа, объединяет все модули
├── video_processor.py    # Модуль для скачивания видео, извлечения аудио и распознавания речи
├── text_structurer.py    # Модуль для обработки текста: очистка, разбиение на секции, структурирование знаний
├── database.py           # Модуль для работы с базой знаний (сохранение, обновление, поиск)
├── utils.py              # Вспомогательные функции и конфигурация
└── requirements.txt      # Зависимости проекта
```

## Требования

- **Python 3.x**
- Пакеты Python: `requests`, `boto3`, `python-dotenv` (dotenv), а также стандартные модули `logging`, `subprocess`, `json`, `time`, `os`
- **ffmpeg** и **ffprobe** (должны быть установлены в системе)
- OAuth-токен для Яндекс.Диска
- API-ключ для Yandex SpeechKit или IAM-токен сервисного аккаунта для SpeechKit
- Доступ к **Yandex Object Storage**

## Установка и настройка

### 1. Клонирование репозитория

```bash
git clone https://github.com/mr_anw/video-to-text.git
cd video-to-text
```

### 2. Создание виртуального окружения в /opt

Рекомендуется запускать сервис в изолированном окружении. Например, создайте каталог для приложения в `/opt` и настройте виртуальное окружение:

```bash
sudo mkdir -p /opt/video-to-text
sudo chown $USER:$USER /opt/video-to-text
cd /opt/video-to-text
python3 -m venv venv
source venv/bin/activate
```

### 3. Установка зависимостей

Находясь в активированном виртуальном окружении, выполните:

```bash
pip install --upgrade pip
pip install -r /path/to/video-to-text/requirements.txt
```

> **Примечание:** Замените `/path/to/video-to-text/` на фактический путь к клонированному репозиторию.

### 4. Создание и настройка файла `.env`

В корневой директории проекта (например, `/path/to/video-to-text`) создайте файл `.env` со следующим содержимым:

```ini
YANDEX_DISK_OAUTH_TOKEN=your_yandex_disk_token
YANDEX_SPEECHKIT_API_KEY=your_speechkit_api_key
YANDEX_SPEECHKIT_IAM_TOKEN=your_service_account_iam_token
YOBJECT_STORAGE_ACCESS_KEY=your_yandex_object_storage_access_key
YOBJECT_STORAGE_SECRET_KEY=your_yandex_object_storage_secret_key
```

### 5. Настройка сервиса systemd для автозапуска

Создайте файл сервиса, например, `/etc/systemd/system/video_to_text.service`:

```ini
[Unit]
Description=Video to Text Processor
After=network.target

[Service]
# Указываем полный путь к Python внутри виртуального окружения
ExecStart=/opt/video-to-text/venv/bin/python /path/to/video-to-text/main.py
WorkingDirectory=/path/to/video-to-text
Restart=always
User=your_user
EnvironmentFile=/path/to/video-to-text/.env

[Install]
WantedBy=multi-user.target
```

> **Примечание:**  
> - Замените `/path/to/video-to-text` на фактический путь к вашему проекту.  
> - Замените `your_user` на имя пользователя, под которым должен запускаться сервис.

### 6. Запуск и проверка сервиса

После настройки сервиса выполните:

```bash
sudo systemctl daemon-reload
sudo systemctl enable video_to_text.service
sudo systemctl start video_to_text.service
sudo systemctl status video_to_text.service
```

## Работа проекта

- **Автоматический запуск:**  
  Скрипт запускается демоном (через systemd) и сканирует указанную папку на Яндекс.Диске каждые **12 часов**.

- **Обработка видеофайлов:**  
  Обрабатываются только новые видеофайлы, которые ранее не были распознаны. Распознанный текст добавляется в файл `recognized_texts.txt`, а обработанные файлы отмечаются в `processed_files.json`.

- **Распознавание аудио:**  
  Поддерживаются два режима распознавания:
  - **general:** Динамический интервал ожидания, рассчитываемый на основе длительности аудио (минимум 10 сек).
  - **deferred-general:** Фиксированный интервал опроса (60 сек) с максимальным временем ожидания 24 часа, что позволяет соблюдать ограничения по количеству запросов к API.

- **Создание базы знаний:**  
  Модуль обработки текста (`text_structurer.py`) очищает расшифрованный текст, разбивает его на логические секции и структурирует в формате вопрос-ответ с классификацией по важности. Итоговая база знаний сохраняется в JSON-файл (например, `knowledge_base.json`) и в будущем будет использоваться для работы AI-ассистента.

## Будущие планы

- **Интеграция с LLaMA:**  
  На следующем этапе планируется запустить LLaMA (с использованием llama.cpp или llama.cpp-python) и обучить модель на базе знаний, созданной из курсов Школы Насти Рыбки. AI-ассистент будет использовать метод Retrieval-Augmented Generation (RAG) для формирования ответов на запросы клиентов.

- **Расширение функционала базы знаний:**  
  Будет реализован модуль `database.py` для более продвинутой работы с данными (поиск, обновление, интеграция с AI-ассистентом).

## Лицензия

Этот проект распространяется под лицензией **MIT License**.

## Контрибьюция

Если вы хотите внести вклад или у вас есть предложения по улучшению проекта, пожалуйста, создайте issue или pull request в репозитории.
```